{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e06f7",
   "metadata": {
    "id": "2eec5cc39a59"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d4ff7",
   "metadata": {
    "id": "d825eda9d6be"
   },
   "source": [
    "# **Initialize BQML Models and Vector Search**\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs two initializations:\n",
    "1. Trains BQML models to predict expenses and detect unusual expenditures for Cymbal Bank Customers\n",
    "2. Indexing the Cymbal Bank website using Vector Search for Retrieval Augmented-Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324a990",
   "metadata": {
    "id": "f3141ab32d1d"
   },
   "source": [
    "### Install Vertex AI SDK and other required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ewxUwrNwugwYhv4i2AjUcS1h",
   "metadata": {
    "id": "ewxUwrNwugwYhv4i2AjUcS1h"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade google-cloud-bigquery\n",
    "!pip install --upgrade langchain nest_asyncio google-cloud-aiplatform\n",
    "!pip install --upgrade google-cloud-aiplatform\n",
    "!pip install tensorflow_hub==0.13.0 tensorflow_text==2.12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdfb468",
   "metadata": {
    "id": "9a0b37a248cd"
   },
   "source": [
    "### Restart runtime\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After it's restarted, continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74255114",
   "metadata": {
    "id": "782dc4469262"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ababc9",
   "metadata": {
    "id": "656aa57d209b"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using Vertex AI Workbench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a580d36d",
   "metadata": {
    "id": "9cad99fe7785"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93af483",
   "metadata": {
    "id": "42a69d0f254f"
   },
   "source": [
    "### Setup Project ID and Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b0ba3a",
   "metadata": {
    "id": "7ec0904aa711"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"your-gcp-project-id\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca18d1",
   "metadata": {
    "id": "3f0b4af8608b"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IBWsAB6tg0B0",
   "metadata": {
    "id": "IBWsAB6tg0B0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Utils\n",
    "import time\n",
    "import urllib.request\n",
    "from typing import List\n",
    "\n",
    "# LangChain\n",
    "import nest_asyncio\n",
    "import requests\n",
    "import vertexai\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import bigquery\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195bcf10",
   "metadata": {
    "id": "a3bf4f3cf9bb"
   },
   "source": [
    "## 1. **BQML Model Initialization**\n",
    "\n",
    "There are two features for which BQML models are used in CymBuddy assistant:\n",
    "1. **Expense Prediction** uses Arima Plus time series forecasting model\n",
    "   - Model is created to predict future expenses per month per user for the following categories:\n",
    "   \n",
    "      | Category | Subcategory|\n",
    "      |----------|------------|\n",
    "      | Earning | Passive |\n",
    "      | Earning | Active |\n",
    "      | Needs | Transportation |\n",
    "      | Needs | Housing |\n",
    "      | Needs | Food and Groceries |\n",
    "      | Needs | Healthcare |\n",
    "      | Needs | Education |\n",
    "      | Wants | Travel |\n",
    "      | Wants | Entertainment |\n",
    "      | Miscellaneous | Miscellaneous |\n",
    "\n",
    "2. **Unusual Expenditure detection** uses Auto Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "FJOFNMp5UE8f",
   "metadata": {
    "id": "FJOFNMp5UE8f"
   },
   "outputs": [],
   "source": [
    "def create_expense_prediction_model(project_id):\n",
    "    client = bigquery.Client()\n",
    "    query_train_arima = f\"\"\"\n",
    "  CREATE OR REPLACE MODEL\n",
    "  `{project_id}.ExpensePrediction.expense_prediction_model` OPTIONS(MODEL_TYPE='ARIMA_PLUS',\n",
    "  TIME_SERIES_TIMESTAMP_COL='month_year',\n",
    "  TIME_SERIES_DATA_COL='transaction_amount',\n",
    "  TIME_SERIES_ID_COL=['ac_id','category','sub_category'],\n",
    "  HOLIDAY_REGION='in') AS\n",
    "  SELECT\n",
    "    month_year, transaction_amount, ac_id, category,sub_category\n",
    "  FROM\n",
    "  `{project_id}.ExpensePrediction.training_data`\n",
    "  \"\"\"\n",
    "    job = client.query(query_train_arima)\n",
    "\n",
    "    # wait for the model to complete training\n",
    "    job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "R24AVZrCUFJH",
   "metadata": {
    "id": "R24AVZrCUFJH"
   },
   "outputs": [],
   "source": [
    "def create_unusual_spends_model(project_id):\n",
    "    client = bigquery.Client()\n",
    "    query_train_arima = f\"\"\"\n",
    "  CREATE MODEL OR REPLACE MODEL `{project_id}.ExpensePrediction.unusual_spend3`\n",
    "  OPTIONS(\n",
    "    model_type='autoencoder',\n",
    "    activation_fn='relu',\n",
    "    batch_size=8,\n",
    "    dropout=0.2,\n",
    "    hidden_units=[32, 16, 4, 16, 32],\n",
    "    learn_rate=0.001,\n",
    "    l1_reg_activation=0.0001,\n",
    "    max_iterations=10,\n",
    "    optimizer='adam'\n",
    "  ) AS\n",
    "  SELECT\n",
    "    transaction_amount, country\n",
    "  FROM\n",
    "    `{project_id}.DummyBankDataset.AccountTransactions`;\n",
    "  \"\"\"\n",
    "    job = client.query(query_train_arima)\n",
    "\n",
    "    # wait for the model to complete training\n",
    "    job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IoggQGoYUYbM",
   "metadata": {
    "id": "IoggQGoYUYbM"
   },
   "outputs": [],
   "source": [
    "create_expense_prediction_model(PROJECT_ID)\n",
    "create_unusual_spends_model(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kGkQM8JMjc_D",
   "metadata": {
    "id": "kGkQM8JMjc_D"
   },
   "source": [
    "## 2. **Initialize Vector Store for Retrieval Augmented-Generation (RAG)**\n",
    "\n",
    "RAG is a technique that helps large language models (LLMs) access and use facts from external sources, for example, websites. This makes their answers more accurate, reliable, and up-to-date. RAG works by finding relevant information based on a question and then giving this information to the language model to help it generate a better answer\n",
    "\n",
    "Following is the RAG architecture used for question answering:\n",
    "\n",
    "![image.png](images/RAG_architecture.png) \n",
    "\n",
    "This colab builds the index in Vector Search by using the following steps:\n",
    "- Read the Cymbal Bank website\n",
    "- Split them in chunks. Website metadata, e.g. website url, is also stored in the chunks\n",
    "- Create embeddings for those chunks\n",
    "    - Store the chunks in a GCS bucket\n",
    "- Build the index in Vector Search using the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa9b2b1",
   "metadata": {
    "id": "26eef38e5d76"
   },
   "source": [
    "### Initialize Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765a6de",
   "metadata": {
    "id": "34e2d7833869"
   },
   "outputs": [],
   "source": [
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a687d",
   "metadata": {
    "id": "62dfcc17e690"
   },
   "source": [
    "### Build the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "S4g2IEu6kQKE",
   "metadata": {
    "id": "S4g2IEu6kQKE"
   },
   "outputs": [],
   "source": [
    "def init_me_libs():\n",
    "    if not os.path.exists(\"utils\"):\n",
    "        os.makedirs(\"utils\")\n",
    "\n",
    "    url_prefix = \"https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/use-cases/document-qa/utils\"\n",
    "    files = [\"__init__.py\", \"matching_engine.py\", \"matching_engine_utils.py\"]\n",
    "\n",
    "    for fname in files:\n",
    "        urllib.request.urlretrieve(f\"{url_prefix}/{fname}\", filename=f\"utils/{fname}\")\n",
    "\n",
    "\n",
    "init_me_libs()\n",
    "\n",
    "from utils.matching_engine import MatchingEngine\n",
    "from utils.matching_engine_utils import MatchingEngineUtils\n",
    "\n",
    "\n",
    "# helper function to list all the urls present on a particular web page\n",
    "def get_urls(url):\n",
    "    reqs = requests.get(url)\n",
    "    soup = BeautifulSoup(reqs.text, \"html.parser\")\n",
    "\n",
    "    urls = []\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        urls.append(link.get(\"href\"))\n",
    "\n",
    "\n",
    "def load_website_content():\n",
    "    nest_asyncio.apply()\n",
    "    website_homepage = \"add-url-of-cloud-run-website\"  # @param {type:\"string\"}\n",
    "\n",
    "    # if all the web pages to be indexed are linked on the website homepage, the following step is sufficient\n",
    "    urls = get_urls(website_homepage)\n",
    "\n",
    "    loader = WebBaseLoader(\n",
    "        urls\n",
    "    )  # add any other urls that need to indexed which are not linked on the website homepage\n",
    "    loader.requests_per_second = 1\n",
    "\n",
    "    documents = loader.aload()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def chunk_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Add chunk number to metadata\n",
    "    for idx, split in enumerate(doc_splits):\n",
    "        split.metadata[\"chunk\"] = idx\n",
    "\n",
    "    print(f\"# of documents = {len(doc_splits)}\")\n",
    "    return doc_splits\n",
    "\n",
    "\n",
    "# Utility functions for Embeddings API with rate limiting\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "class BaseModelMixin:\n",
    "    requests_per_minute: int\n",
    "    num_instances_per_batch: int\n",
    "\n",
    "    # Overriding embed_documents method\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        docs = list(texts)\n",
    "\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[: self.num_instances_per_batch],\n",
    "                docs[self.num_instances_per_batch :],\n",
    "            )\n",
    "            chunk = self.client.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]\n",
    "\n",
    "\n",
    "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModelMixin):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mqy90iFMjpLG",
   "metadata": {
    "id": "Mqy90iFMjpLG"
   },
   "outputs": [],
   "source": [
    "# Embeddings API integrated with langChain\n",
    "EMBEDDING_QPM = 100\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    ")\n",
    "\n",
    "ME_REGION = REGION\n",
    "ME_INDEX_NAME = f\"{PROJECT_ID}-me-index\"\n",
    "ME_EMBEDDING_DIR = f\"{PROJECT_ID}-me-bucket\"\n",
    "ME_DIMENSIONS = 768  # when using Vertex PaLM Embedding\n",
    "\n",
    "m_engine = MatchingEngineUtils(PROJECT_ID, ME_REGION, ME_INDEX_NAME)\n",
    "\n",
    "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = m_engine.get_index_and_endpoint()\n",
    "print(f\"ME_INDEX_ID={ME_INDEX_ID}\")\n",
    "print(f\"ME_INDEX_ENDPOINT_ID={ME_INDEX_ENDPOINT_ID}\")\n",
    "\n",
    "# initialize vector store\n",
    "me = MatchingEngine.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=ME_REGION,\n",
    "    gcs_bucket_name=f\"gs://{ME_EMBEDDING_DIR}\".split(\"/\")[2],\n",
    "    embedding=embeddings,\n",
    "    index_id=ME_INDEX_ID,\n",
    "    endpoint_id=ME_INDEX_ENDPOINT_ID,\n",
    ")\n",
    "\n",
    "documents = load_website_content()\n",
    "\n",
    "doc_splits = chunk_documents(documents)\n",
    "# Store docs as embeddings in Matching Engine index\n",
    "# It may take a while since API is rate limited\n",
    "texts = [doc.page_content for doc in doc_splits]\n",
    "metadatas = [\n",
    "    [\n",
    "        {\"namespace\": \"source\", \"allow_list\": [doc.metadata[\"source\"]]},\n",
    "        {\"namespace\": \"title\", \"allow_list\": [doc.metadata[\"title\"]]},\n",
    "        {\"namespace\": \"chunk\", \"allow_list\": [str(doc.metadata[\"chunk\"])]},\n",
    "    ]\n",
    "    for doc in doc_splits\n",
    "]\n",
    "\n",
    "doc_ids = me.add_texts(texts=texts, metadatas=metadatas)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "init_bqml_and_vector_search.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
