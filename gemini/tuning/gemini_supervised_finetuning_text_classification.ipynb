{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Text Classification with Gemini\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/gemini_supervised_finetuning_text_classification.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Ftuning%2Fgemini_supervised_finetuning_text_classification.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/tuning/gemini_supervised_finetuning_text_classification.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/gemini_supervised_finetuning_text_classification.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy3a0zd7phg8"
   },
   "source": [
    "| | | |\n",
    "|-|-|-|\n",
    "|Author(s) | [Gabriela Hernandez Larios](https://github.com/gabrielahrlr) | [Elia Secchi](https://github.com/eliasecchig)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to perform text classification with Gemini models. From in-context learning (using zero-shot and few-shot learning) to in-weights learning fine-tuning Gemini models for text classification.\n",
    "\n",
    "### Objective\n",
    " We'll cover the development cycle from preparing the dataset, to setting up an evaluation framework to perform text classification tasks using Gemini. Additionally, you'll learn how to create and log experiments, adapting Gemini models to the text classification task with in-context and in-weights (fine-tuning) learning approaches, and compare the performances.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML Services and Resources:\n",
    "\n",
    "- Google Cloud Storage\n",
    "- Vertex AI Experiments\n",
    "- Vertex AI Tuning\n",
    "- Gemini 1.0 Pro, Gemini 1.5 Pro and Gemini 1.5 Flash\n",
    "\n",
    "The steps performed include:\n",
    "- [Load and split dataset](#scrollTo=EdvJRUWRNGHE&line=1&uniqifier=1)\n",
    "- [Evaluation and Experiment Setup](#scrollTo=c2YOsromfcuB&line=6&uniqifier=1)\n",
    "- [In-Context learning (zero-shot and few-shot) with Gemini Model](#scrollTo=EfKnRU-SfcuB)\n",
    "- [Fine-tuning Gemini 1.0 Pro for text classification](#scrollTo=Qs9eHiL5fcuD)\n",
    "- [Comparative Evaluation]()\n",
    "- [[Optional] Heuristics for computing Confidence Scores](#scrollTo=KW7wPWQWuQT4)\n",
    "\n",
    "### Dataset\n",
    "The [BBC News dataset](http://mlg.ucd.ie/datasets/bbc.html) consists 2225 articles from the BBC news website corresponding to five topical areas: business, entertainment, politics, sport, and tech.  This dataset was downloaded from http://mlg.ucd.ie/datasets/bbc.html\n",
    "\n",
    "**Dataset Citation**\n",
    "\n",
    "```\n",
    "@inproceedings{greene06icml,\n",
    "\tAuthor = {Derek Greene and P\\'{a}draig Cunningham},\n",
    "\tBooktitle = {Proc. 23rd International Conference on Machine learning (ICML'06)},\n",
    "\tPages = {377--384},\n",
    "\tPublisher = {ACM Press},\n",
    "\tTitle = {Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering},\n",
    "\tYear = {2006}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform\n",
    "%pip install --user --quiet datasets\n",
    "%pip install --user --quiet backoff\n",
    "%pip install --user --quiet multiprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart runtime (Colab only)\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILS7NNudfct_"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjsxAjgJfct_"
   },
   "outputs": [],
   "source": [
    "import backoff\n",
    "import multiprocess as mp\n",
    "import traceback\n",
    "import nest_asyncio\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import vertexai\n",
    "\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Any, Union\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score\n",
    "from typing import Dict\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.generative_models import GenerativeModel, HarmCategory, HarmBlockThreshold\n",
    "from vertexai.preview.tuning import sft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information, initialize Vertex AI SDK for Python and create a GCS bucket\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5r1Bhf7fcuA"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[yout-bucket-name]]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSZS8QfOfcuA"
   },
   "source": [
    "**warning:** Only if your bucket doesn't already exist: Run the following cell to create your Cloud Storage bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXxi2tC2fcuA"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $LOCATION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSZ8-hwafcuA"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqCIzmWSfcuA"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAEAlYH2fcuA"
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6sph8etfcuA"
   },
   "source": [
    "#### Batch Prediction - Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fz4tGANnfcuA"
   },
   "outputs": [],
   "source": [
    "def backoff_hdlr(details):\n",
    "    print(\"Backing off {wait:0.1f} seconds after {tries} tries \".format(**details))\n",
    "\n",
    "\n",
    "def log_error(msg, *args):\n",
    "    mp.get_logger().error(msg, *args)\n",
    "    raise Exception(msg)\n",
    "\n",
    "\n",
    "def handle_exception_threading(f):\n",
    "    def applicator(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            log_error(traceback.format_exc())\n",
    "\n",
    "    return applicator\n",
    "\n",
    "\n",
    "@handle_exception_threading\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, ResourceExhausted, max_tries=30, on_backoff=backoff_hdlr\n",
    ")\n",
    "def _predict_message(message: str, model: GenerativeModel):\n",
    "    \"\"\"\n",
    "    Predict messages\n",
    "    \"\"\"\n",
    "    response = model.generate_content([message], stream=False)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def batch_predict(messages: list, model: GenerativeModel, max_workers: int = 4) -> list:\n",
    "    \"\"\"\n",
    "    Predicts the classes for a list of messages\n",
    "\n",
    "    Args:\n",
    "        - messages: list of all messages to predict\n",
    "        - model_name: model to use for predicting.\n",
    "        - max_workers: number of workers to use for parallel predictions\n",
    "\n",
    "    Returns:\n",
    "        - list of predicted labels\n",
    "\n",
    "    \"\"\"\n",
    "    predictions = list()\n",
    "    with ThreadPoolExecutor(max_workers) as pool:\n",
    "        partial_func = partial(_predict_message, model=model)\n",
    "        for message in tqdm(pool.map(partial_func, messages), total=len(messages)):\n",
    "            predictions.append(message)\n",
    "            pass\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tKVjsJKfcuA"
   },
   "source": [
    "#### Experiment Logging - Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJ5BM7Z5fcuA"
   },
   "outputs": [],
   "source": [
    "def create_experiment(\n",
    "    experiment_name: str,\n",
    "    project: str,\n",
    "    location: str,\n",
    "    experiment_description: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an Experiment on Vertex AI Experiments\n",
    "    Args:\n",
    "        - experiment_name: Provide a name for your experiment.\n",
    "        - experiment_description: Provide a description for your experiment.\n",
    "        - project: Your project ID. You can find these IDs in the Google Cloud console welcome page.\n",
    "        - location: See List of available locations Be sure to use a region that supports TensorBoard if creating a TensorBoard instance.\n",
    "    \"\"\"\n",
    "    aiplatform.init(\n",
    "        experiment=experiment_name,\n",
    "        experiment_description=experiment_description,\n",
    "        experiment_tensorboard=False,\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "\n",
    "\n",
    "def log_experiment_run_vertexai(\n",
    "    experiment_name: str,\n",
    "    run_name: str,\n",
    "    params: Dict[str, Union[float, int, str]],\n",
    "    metrics: Dict[str, float],\n",
    "    project: str,\n",
    "    location: str,\n",
    "):\n",
    "    aiplatform.init(experiment=experiment_name, project=project, location=location)\n",
    "    aiplatform.start_run(run=run_name)\n",
    "    aiplatform.log_params(params)\n",
    "    aiplatform.log_metrics(metrics)\n",
    "    aiplatform.end_run()\n",
    "\n",
    "\n",
    "def get_experiments_data_frame_sample(\n",
    "    experiment: str,\n",
    "    project: str,\n",
    "    location: str,\n",
    "):\n",
    "    aiplatform.init(experiment=experiment, project=project, location=location)\n",
    "    experiments_df = aiplatform.get_experiment_df()\n",
    "    return experiments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "## 1. Load and Splitting Dataset\n",
    "In this step, we will load the raw data and create training, validation and test sets. Later these datasets will be used to perform  different types of adaptations to Gemini models for the task under consideration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k70Yb_XFfcuA"
   },
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"SetFit/bbc-news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAi4NI2MfcuA"
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame(datasets[\"train\"])\n",
    "test = pd.DataFrame(datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcTrjjGTfcuA"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSfs2R-dfcuB"
   },
   "outputs": [],
   "source": [
    "train.label_text.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmAsTaDIfcuB"
   },
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRYqeB-kfcuB"
   },
   "outputs": [],
   "source": [
    "val, test = train_test_split(\n",
    "    test, test_size=0.5, shuffle=True, stratify=test[\"label_text\"], random_state=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRtiloYgfcuB"
   },
   "outputs": [],
   "source": [
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySB5n2ZwfcuB"
   },
   "outputs": [],
   "source": [
    "val.label_text.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec1u71uzfcuB"
   },
   "outputs": [],
   "source": [
    "test.label_text.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2YOsromfcuB"
   },
   "source": [
    "##  2. Evaluation and Experiment Setup\n",
    "We will create the required functions to evaluate our experiments and to log them in Vertex Experiments.\n",
    "\n",
    "\n",
    "### Evaluation Setup\n",
    "For this text classification task, we will use the below classification metrics to evaluate the performance of the models and it different adaptations. We will track the below metrics in our development.\n",
    "\n",
    "- Overall Micro-F1\n",
    "- Overall Macro-F1\n",
    "- Overall Accuracy\n",
    "- Overall Weighted Precision\n",
    "- Overall Weighted Recall\n",
    "- F1-Score (overall and per class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWbdf4gmfcuB"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "\n",
    "def predictions_postprocessing(text):\n",
    "    \"\"\"\n",
    "    Cleans the predicted class label string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The predicted class label string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned class label string.\n",
    "    \"\"\"\n",
    "    text = text.rstrip()\n",
    "    text = text.lstrip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def evaluate_predictions(\n",
    "    df: pd.DataFrame,\n",
    "    target_column: str = \"label_text\",\n",
    "    predictions_column: str = \"predicted_labels\",\n",
    "    postprocessing: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\" \"\n",
    "    Batch evaluation of predictions, returns a dictionary with the metric\n",
    "    Args:\n",
    "       - df (pandas.DataFrame):  a pandas dataframe with two mandatory columns, a target column with\n",
    "       the actual true values, and a predictions column with the predicted values.\n",
    "       - target_column (str): column name with the actual ground truth values\n",
    "       - predictions_column (str): column name with the model predictions\n",
    "    \"\"\"\n",
    "\n",
    "    if postprocessing:\n",
    "        df[predictions_column] = df[predictions_column].apply(\n",
    "            lambda x: predictions_postprocessing(x)\n",
    "        )\n",
    "\n",
    "    metrics_report = classification_report(\n",
    "        df[target_column], df[predictions_column], output_dict=True\n",
    "    )\n",
    "    overall_macro_f1_score = f1_score(\n",
    "        df[target_column], df[predictions_column], average=\"macro\"\n",
    "    )\n",
    "    overall_micro_f1_score = f1_score(\n",
    "        df[target_column], df[predictions_column], average=\"micro\"\n",
    "    )\n",
    "    weighted_precision = precision_score(\n",
    "        df[target_column], df[predictions_column], average=\"weighted\"\n",
    "    )\n",
    "    weighted_recall = recall_score(\n",
    "        df[target_column], df[predictions_column], average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": metrics_report[\"accuracy\"],\n",
    "        \"weighted precision\": weighted_precision,\n",
    "        \"weighted recall\": weighted_recall,\n",
    "        \"macro f1\": overall_macro_f1_score,\n",
    "        \"micro f1\": overall_micro_f1_score,\n",
    "        \"business_f1_score\": metrics_report[\"business\"][\"f1-score\"],\n",
    "        \"sport_f1_score\": metrics_report[\"sport\"][\"f1-score\"],\n",
    "        \"politics_f1_score\": metrics_report[\"politics\"][\"f1-score\"],\n",
    "        \"tech_f1_score\": metrics_report[\"tech\"][\"f1-score\"],\n",
    "        \"entertainment_f1_score\": metrics_report[\"entertainment\"][\"f1-score\"],\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g3sHozyfcuB"
   },
   "source": [
    "### Experiment Setup\n",
    "Before starting the development and experimentation process, we will setup Vertex AI Experiments, in order to log all the experiments we run and compare them based on the estipulated metrics. Some of the functions for setting up vertex experiments are in the [helper functions section](#scrollTo=0tKVjsJKfcuA). In this part we will just create an experiment where we will log all our different runs.\n",
    "\n",
    "For more information about Vertex Experiments, please refer to its [documentation](https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9wT-nkQOeik"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"[your-experiment]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYWhf5YTfcuB"
   },
   "outputs": [],
   "source": [
    "create_experiment(\n",
    "    experiment_name=EXPERIMENT_NAME, project=PROJECT_ID, location=LOCATION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nQymMebwJ-Y"
   },
   "outputs": [],
   "source": [
    "## Create an Evaluation dataframe yo store the predictions from all the experiments.\n",
    "df_evals = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfKnRU-SfcuB"
   },
   "source": [
    "## 3. In-Context Adaptation using Gemini models\n",
    "\n",
    "In this section we'll do in-context learning to instruct Gemini models to perform the text classification task under consideration, using zero-shot and few-shot prompt engineering techniques.  We'll use and compare the performances of Gemini 1.0 Pro, Gemini 1.5 Pro and Gemini 1.5 Flash using the same prompts.\n",
    "\n",
    "The prompts presented in this section are crafted for this task, and in our experiments they demonstrate superior results compared to other simpler prompts.\n",
    "\n",
    "**Before fine-tuning a model, it is important to find the best prompt**: system instructions, examples, structure, etc., for the task under consideration. This will permit to get an understanding of which prompt works the best for the used model, and even boost more the performances when fine-tuning.\n",
    "\n",
    "**Note:** Prompt Engineering is model-dependent. We recommend you to experiment with different prompting techniques per model. Techniques like Chain-of-Thought can increase performances, as well as Dynamic Few-Shots (using a RAG system to dynamically integrate the examples that are similar to the user input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FO5XWYDBfcuB"
   },
   "outputs": [],
   "source": [
    "system_prompt_zero_shot = \"\"\"TASK:\n",
    "Classify the text into ONLY one of the following classes [business, entertainment, politics, sport, tech].\n",
    "\n",
    "CLASSES:\n",
    "- business\n",
    "- entertainment\n",
    "- politics\n",
    "- sport\n",
    "- tech\n",
    "\n",
    "INSTRUCTIONS\n",
    "- Respond with ONLY one class.\n",
    "- You MUST use the exact word from the list above.\n",
    "- DO NOT create or use any other classes.\n",
    "- CAREFULLY analyze the text before choosing the best-fitting category from [business, entertainment, politics, sport, tech].\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ne1zh7hfcuB"
   },
   "source": [
    "For the few-shot prompt, we'll randomly pick an example from each category using the `train` dataset we previously computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FApEzz62fcuB"
   },
   "outputs": [],
   "source": [
    "system_prompt_few_shot = f\"\"\"TASK:\n",
    "Classify the text into ONLY one of the following classes [business, entertainment, politics, sport, tech].\n",
    "\n",
    "CLASSES:\n",
    "- business\n",
    "- entertainment\n",
    "- politics\n",
    "- sport\n",
    "- tech\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Respond with ONLY one class.\n",
    "- You MUST use the exact word from the list above.\n",
    "- DO NOT create or use any other classes.\n",
    "- CAREFULLY analyze the text before choosing the best-fitting category from [business, entertainment, politics, sport, tech].\n",
    "\n",
    "EXAMPLES:\n",
    "- EXAMPLE 1:\n",
    "    <user>\n",
    "    {train.loc[train[\"label_text\"] == \"business\", \"text\"].iloc[10]}\n",
    "    <model>\n",
    "    {train.loc[train[\"label_text\"] == \"business\", \"label_text\"].iloc[10]}\n",
    "\n",
    "- EXAMPLE 2:\n",
    "    <user>\n",
    "    {train.loc[train[\"label_text\"] == \"entertainment\", \"text\"].iloc[10]}\n",
    "    <model>\n",
    "    {train.loc[train[\"label_text\"] == \"entertainment\", \"label_text\"].iloc[10]}\n",
    "\n",
    "- EXAMPLE 3:\n",
    "    <user>\n",
    "    {train.loc[train[\"label_text\"] == \"politics\", \"text\"].iloc[10]}\n",
    "    <model>\n",
    "    {train.loc[train[\"label_text\"] == \"politics\", \"label_text\"].iloc[10]}\n",
    "\n",
    "- EXAMPLE 4:\n",
    "    <user>\n",
    "    {train.loc[train[\"label_text\"] == \"sport\", \"text\"].iloc[10]}\n",
    "    <model>\n",
    "    {train.loc[train[\"label_text\"] == \"sport\", \"label_text\"].iloc[10]}\n",
    "\n",
    "- EXAMPLE 4:\n",
    "    <user>\n",
    "    {train.loc[train[\"label_text\"] == \"tech\", \"text\"].iloc[10]}\n",
    "    <model>\n",
    "    {train.loc[train[\"label_text\"] == \"tech\", \"label_text\"].iloc[10]}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMVdJ2EffcuC"
   },
   "source": [
    "For the below evaluations, we'll use the respective functions we have already set up. For in-context learning, we recommend to use the validation set to find the optimal performance and then apply it to the test set, to make sure the metrics remain consistent. In this notebook, we'll directly evaluate on the test dataset, as the validation and prompt engineering part has been already done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF_tas6BfcuC"
   },
   "source": [
    "### 3.1 Gemini 1.0 Pro in-context Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLitVLOFqPQe"
   },
   "outputs": [],
   "source": [
    "generation_config = {\"max_output_tokens\": 10, \"temperature\": 0}\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "gem_pro_1_model_zero = GenerativeModel(\n",
    "    \"gemini-1.0-pro-002\",\n",
    "    system_instruction=[system_prompt_zero_shot],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1D2vrHjawsZH"
   },
   "outputs": [],
   "source": [
    "# Get the list of messages to predict\n",
    "messages_to_predict = test[\"text\"].to_list()\n",
    "# Compute the preictions\n",
    "predictions_zero_shot = batch_predict(\n",
    "    messages=messages_to_predict, model=gem_pro_1_model_zero, max_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufTv6Jx0wwIZ"
   },
   "outputs": [],
   "source": [
    "df_evals[\"gem1.0-zero-shot_predictions\"] = predictions_zero_shot\n",
    "len(predictions_zero_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0h3Tr0LwwBX"
   },
   "outputs": [],
   "source": [
    "# Compute Evaluation Metrics for zero-shot prompt\n",
    "metrics_zero_shot = evaluate_predictions(\n",
    "    df_evals.copy(),\n",
    "    target_column=\"label_text\",\n",
    "    predictions_column=\"gem1.0-zero-shot_predictions\",\n",
    "    postprocessing=True,\n",
    ")\n",
    "metrics_zero_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aa6HgjDNwv4y"
   },
   "outputs": [],
   "source": [
    "# Log Experiment with zero-shot Prompt with Gemini 1.0 Pro\n",
    "params = {\n",
    "    \"model\": \"gemini-1.0-pro-002\",\n",
    "    \"adaptation_type\": \"in-context zero-shot\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_output_tokens\": 10,\n",
    "}\n",
    "\n",
    "log_experiment_run_vertexai(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    run_name=\"gemini-1-0-pro-002-zero-shot\",\n",
    "    params=params,\n",
    "    metrics=metrics_zero_shot,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Znl2x6ZSw7cs"
   },
   "outputs": [],
   "source": [
    "# Test Few-Shot, and other prompts/possibilities\n",
    "gem_pro_1_model_few = GenerativeModel(\n",
    "    \"gemini-1.0-pro-002\",\n",
    "    system_instruction=[system_prompt_few_shot],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NM7OU8hZfcuC"
   },
   "outputs": [],
   "source": [
    "predictions_few_shot = batch_predict(\n",
    "    messages=messages_to_predict, model=gem_pro_1_model_few\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwUABEdnfcuC"
   },
   "outputs": [],
   "source": [
    "df_evals[\"gem1.0-few-shot_predictions\"] = predictions_few_shot\n",
    "len(predictions_few_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6CrJ1NzfcuC"
   },
   "outputs": [],
   "source": [
    "# Compute Evaluation Metrics for few-shot prompt\n",
    "metrics_few_shot = evaluate_predictions(\n",
    "    df_evals.copy(),\n",
    "    target_column=\"label_text\",\n",
    "    predictions_column=\"gem1.0-few-shot_predictions\",\n",
    "    postprocessing=True,\n",
    ")\n",
    "metrics_few_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qvex8Jd2fcuC"
   },
   "outputs": [],
   "source": [
    "# Log Experiment with Few-Shot Prompt with Gemini 1.0 Pro\n",
    "\n",
    "params = {\n",
    "    \"model\": \"gemini-1.0-pro-002\",\n",
    "    \"adaptation_type\": \"in-context few-shot\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_output_tokens\": 10,\n",
    "}\n",
    "\n",
    "log_experiment_run_vertexai(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    run_name=\"gemini-1-0-pro-few-shot\",\n",
    "    params=params,\n",
    "    metrics=metrics_few_shot,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQ1fbnFEfcuC"
   },
   "source": [
    "### 3.2 Gemini 1.5 Pro in-context Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1kLB6PaxJij"
   },
   "outputs": [],
   "source": [
    "# Define the model and configurations\n",
    "generation_config = {\"max_output_tokens\": 10, \"temperature\": 0}\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "gem_pro_1_5_model_zero = GenerativeModel(\n",
    "    \"gemini-1.5-pro-001\",\n",
    "    system_instruction=[system_prompt_zero_shot],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cdpWcimfcuC"
   },
   "outputs": [],
   "source": [
    "# Get the list of messages to predict\n",
    "messages_to_predict = test[\"text\"].sample(50).to_list()\n",
    "# Compute the predictions using the zero-shot prompt\n",
    "predictions_zero_shot_15_pro = batch_predict(\n",
    "    messages=messages_to_predict, model=gem_pro_1_5_model_zero\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCjUhhHMfcuC"
   },
   "outputs": [],
   "source": [
    "df_evals[\"gem1.5-pro-zero-shot_predictions\"] = predictions_zero_shot_15_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArRBdMEVfcuC"
   },
   "outputs": [],
   "source": [
    "# Compute Evaluation Metrics for zero-shot prompt\n",
    "metrics_zero_shot_15_pro = evaluate_predictions(\n",
    "    df_evals.copy(),\n",
    "    target_column=\"label_text\",\n",
    "    predictions_column=\"gem1.5-pro-zero-shot_predictions\",\n",
    "    postprocessing=True,\n",
    ")\n",
    "metrics_zero_shot_15_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnfziX3IfcuC"
   },
   "outputs": [],
   "source": [
    "# Log Experiment with zero-shot Prompt with Gemini 1.5 Pro\n",
    "\n",
    "params = {\n",
    "    \"model\": \"gemini-1.5-pro-001\",\n",
    "    \"adaptation_type\": \"in-context zero-shot\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_output_tokens\": 10,\n",
    "}\n",
    "\n",
    "log_experiment_run_vertexai(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    run_name=\"gemini-1-5-pro-zero-shot\",\n",
    "    params=params,\n",
    "    metrics=metrics_zero_shot_15_pro,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0S4YWw7GfcuC"
   },
   "source": [
    "### 3.3 Gemini 1.5 Flash in-context Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3LEXUTbxrGE"
   },
   "outputs": [],
   "source": [
    "# Define the model and configurations\n",
    "generation_config = {\"max_output_tokens\": 10, \"temperature\": 0}\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "gem_1_5_flash_model_zero = GenerativeModel(\n",
    "    \"gemini-1.5-flash-001\",\n",
    "    system_instruction=[system_prompt_zero_shot],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndrfuY5NfcuC"
   },
   "outputs": [],
   "source": [
    "# Get the list of messages to predict\n",
    "messages_to_predict = test[\"text\"].to_list()\n",
    "# Compute the predictions using the zero-shot prompt\n",
    "predictions_zero_shot_15_flash = batch_predict(\n",
    "    messages=messages_to_predict, model=gem_1_5_flash_model_zero\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liLDktRGfcuC"
   },
   "outputs": [],
   "source": [
    "df_evals[\"gem1.5-flash-zero-shot_predictions\"] = predictions_zero_shot_15_flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fjbxbHkfcuC"
   },
   "outputs": [],
   "source": [
    "# Compute Evaluation Metrics for zero-shot prompt\n",
    "metrics_zero_shot_15_flash = evaluate_predictions(\n",
    "    df_evals.copy(),\n",
    "    target_column=\"label_text\",\n",
    "    predictions_column=\"gem1.5-flash-zero-shot_predictions\",\n",
    "    postprocessing=True,\n",
    ")\n",
    "metrics_zero_shot_15_flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceSBZ36EfcuC"
   },
   "outputs": [],
   "source": [
    "# Log Experiment with zero-shot Prompt with Gemini 1.5 Pro\n",
    "\n",
    "params = {\n",
    "    \"model\": \"gemini-1.5-flash-001\",\n",
    "    \"adaptation_type\": \"in-context zero-shot\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_output_tokens\": 10,\n",
    "}\n",
    "\n",
    "log_experiment_run_vertexai(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    run_name=\"gemini-1-5-flash-zero-shot\",\n",
    "    metrics=metrics_zero_shot_15_flash,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qs9eHiL5fcuD"
   },
   "source": [
    "## 4. Fine-tuning (Parameter Efficient) Gemini Pro 1.0\n",
    "Supervised fine-tuning helps adapt foundation models to new tasks using smaller, highly relevant datasets. To ensure success, focus on:\n",
    "\n",
    "- Using domain-specific data: Choose data closely matching your real-world use case.\n",
    "- Accurate labeling: High-quality annotations are crucial.\n",
    "- Clean data: Remove duplicates, fix errors, and ensure relevance to your task.\n",
    "- Diverse but focused examples: Include variety within your target domain, avoiding irrelevant data.\n",
    "- Balanced classes (for classification): Maintain a balance to prevent bias towards a specific class.\n",
    "\n",
    "### 4.1 Prepare tuning datasets for fine-tuning Gemini Models on Vertex AI\n",
    "\n",
    "Training data should be structured within a JSONL file located at a Google Cloud Storage (GCS) URI. Each line (or row) of the JSONL file must adhere to a specific schema: It should contain a \"messages\" array, with objects inside defining a \"role\" (\"system\" for the system context,  \"user\" for user input or \"model\" for model output) and the corresponding text \"content\". For example, a valid data row would look like this:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You should classify the text into one of the following classes:[business, entertainment]\"\n",
    "      },\n",
    "      { \"role\": \"user\", \"content\": \"Diversify your investment portfolio\" },\n",
    "      { \"role\": \"model\", \"content\": \"business\" }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "The role \"system\" is optional. You can find more information about the dataset format and preparation in the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-about)\n",
    "\n",
    "To run a tuning job, you need to upload your tuning and validation(optional) datasets to a Cloud Storage bucket. You can either create a new Cloud Storage bucket or use an existing one to store dataset files. We recommend that you use a bucket that's in the same Google Cloud project where you plan to tune your model.\n",
    "\n",
    "\n",
    "In this section, we will provide guidelines to prepare the training and validation (optional) datasets based on three options:\n",
    "\n",
    "1. [Option 1] From scratch, using the datasets we loaded and splitted at the beginning of this notebook.\n",
    "\n",
    "1. [Option 2] Providing a function to convert an AutoML Dataset on CSV format to the expected format to fine-tune and validate Gemini Models.\n",
    "\n",
    "1. [Option 3] Providing a function to convert an AutoML Dataset on JSONL format to the expected format to fine-tune and validate Gemini Models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvhGIruMfcuD"
   },
   "source": [
    "#### [Option 1] Prepare tuning and validation datasets from scratch\n",
    "\n",
    "We need to prepare our training and validaiton (optional) datasets for the text classification task. It is recommended to ass a system role with the instructions on how to classify as part of the dataset. Since we are going to fine-tune the model, the need to add few-shot examples as part of the prompt is eliminated, and therefore we will reuse the `system_prompt_zero_shot` that we used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vu8TAIqUfcuD"
   },
   "outputs": [],
   "source": [
    "# Create tuning dataset\n",
    "# shuffle the data\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "gemini_tuning_dataset = list()\n",
    "for idx, row in train.iterrows():\n",
    "    message_system = {\"role\": \"system\", \"content\": system_prompt_zero_shot}\n",
    "    message_user = {\"role\": \"user\", \"content\": row[\"text\"]}\n",
    "    message_model = {\"role\": \"model\", \"content\": row[\"label_text\"]}\n",
    "    gemini_tuning_dataset.append(\n",
    "        {\"messages\": [message_system, message_user, message_model]}\n",
    "    )\n",
    "\n",
    "gemini_tuning_df = pd.DataFrame(gemini_tuning_dataset)\n",
    "print(gemini_tuning_df.shape)\n",
    "gemini_tuning_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQQM8MZNfcuD"
   },
   "outputs": [],
   "source": [
    "# store tuning dataset in GCS\n",
    "tuning_data_gcs_path = f\"gs://{BUCKET_NAME}/tuning_experiments/tuning_dataset_gemini.jsonl\"  # @param {type: \"string\"}\n",
    "\n",
    "gemini_tuning_df.to_json(tuning_data_gcs_path, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcPBW260fcuD"
   },
   "outputs": [],
   "source": [
    "# Create validation dataset\n",
    "gemini_validation_dataset = list()\n",
    "# shuffle the data\n",
    "val_sampled = val.sample(frac=1).reset_index(drop=True)\n",
    "for idx, row in val_sampled.iterrows():\n",
    "    message_system = {\"role\": \"system\", \"content\": system_prompt_zero_shot}\n",
    "    message_user = {\"role\": \"user\", \"content\": row[\"text\"]}\n",
    "    message_model = {\"role\": \"model\", \"content\": row[\"label_text\"]}\n",
    "    gemini_validation_dataset.append(\n",
    "        {\"messages\": [message_system, message_user, message_model]}\n",
    "    )\n",
    "\n",
    "gemini_validation_df = pd.DataFrame(gemini_validation_dataset)\n",
    "print(gemini_validation_df.shape)\n",
    "gemini_validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGnLQB24fcuD"
   },
   "outputs": [],
   "source": [
    "# store validation dataset in GCS\n",
    "validation_data_gcs_path = f\"gs://{BUCKET_NAME}/tuning_experiments/validation_dataset_gemini.jsonl\"  # @param {type: \"string\"}\n",
    "gemini_validation_df.to_json(validation_data_gcs_path, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D4sezIqfcuD"
   },
   "source": [
    "#### [Option 2] AutoML CSV training dataset format to Gemini tuning data format.\n",
    "If you were previously using Vertex AI AutoML for text classification, and you have your data in the below csv format expected by AutoML:\n",
    "\n",
    "```\n",
    "[ml_use],gcs_file_uri|\"inline_text\",label\n",
    "```\n",
    "\n",
    "```\n",
    "test,\"inline_text\",label1\n",
    "test,\"inline_text\",label2\n",
    "training,\"inline_text\",label3\n",
    "validation,\"inline_text\",label1\n",
    "```\n",
    "\n",
    "Youu can utilize the below function to convert your AutoML CSV datasets for text classification to the format expected for the training datasets to fine-tune Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Egqv8KlsyKFA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_automl_csv_dataset_to_gemini(\n",
    "    automl_gcs_csv_path: str, system_prompt: str = \"\", partition=\"training\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts an AutoML CSV dataset for text classification to the Gemini tuning format.\n",
    "\n",
    "    This function takes an AutoML CSV dataset path on Google Cloud Storage and a partition name as input.\n",
    "    It reads the CSV file, filters the data based on the partition, and then\n",
    "    converts it into the Gemini tuning format. The Gemini tuning format requires a\n",
    "    list of dictionaries, where each dictionary represents a conversation turn\n",
    "    with \"role\" and \"content\" keys.\n",
    "\n",
    "    Args:\n",
    "      automl_gcs_csv_path: The GCS path to the AutoML CSV dataset.\n",
    "      system_prompt: the instructions to the model\n",
    "      partition: The partition to extract from the dataset (e.g., \"training\",\n",
    "        \"validation\", \"test\"). Defaults to \"training\".\n",
    "\n",
    "    Returns:\n",
    "      A pandas DataFrame containing the data in the Gemini tuning format.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(gcs_path_automl_dataset, names=[\"partition\", \"text\", \"label\"])\n",
    "\n",
    "    df_automl = df.loc[df[\"partition\"] == partition]\n",
    "    gemini_dataset = list()\n",
    "\n",
    "    if system_prompt != \"\":\n",
    "        for idx, row in df_automl.iterrows():\n",
    "            message_system = {\"role\": \"system\", \"content\": system_prompt}\n",
    "            message_user = {\"role\": \"user\", \"content\": row[\"text\"]}\n",
    "            message_model = {\"role\": \"model\", \"content\": row[\"label\"]}\n",
    "            gemini_dataset.append(\n",
    "                {\"messages\": [message_system, message_user, message_model]}\n",
    "            )\n",
    "    else:\n",
    "        for idx, row in df_automl.iterrows():\n",
    "            message_user = {\"role\": \"user\", \"content\": row[\"text\"]}\n",
    "            message_model = {\"role\": \"model\", \"content\": row[\"label\"]}\n",
    "            gemini_dataset.append(\n",
    "                {\"messages\": [message_system, message_user, message_model]}\n",
    "            )\n",
    "\n",
    "    df_gemini = pd.DataFrame(gemini_dataset)\n",
    "    return df_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5mzsiDl0CMq"
   },
   "outputs": [],
   "source": [
    "## Usage Example for Training dataset\n",
    "gcs_path_automl_dataset = (\n",
    "    \"gs://<your-bucket-path>/<your-data>.csv\"  # @param {type: \"string\"}\n",
    ")\n",
    "df_gemini_tuning = convert_automl_csv_dataset_to_gemini(\n",
    "    automl_gcs_csv_path=gcs_path_automl_dataset,\n",
    "    system_prompt=system_prompt_zero_shot,\n",
    "    partition=\"training\",\n",
    ")\n",
    "\n",
    "# store tuning dataset in GCS\n",
    "gemini_tuning_data_gcs_path = f\"gs://{BUCKET_NAME}/tuning_experiments/tuning_dataset_gemini.jsonl\"  # @param {type: \"string\"}\n",
    "df_gemini_tuning.to_json(gemini_tuning_data_gcs_path, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIBlBE2q0yYp"
   },
   "outputs": [],
   "source": [
    "## Usage Example for validation dataset\n",
    "gcs_path_automl_dataset = (\n",
    "    \"gs://<your-bucket-path>/<your-data>.csv\"  # @param {type: \"string\"}\n",
    ")\n",
    "df_gemini_validation = convert_automl_csv_dataset_to_gemini(\n",
    "    automl_gcs_csv_path=gcs_path_automl_dataset,\n",
    "    system_prompt=system_prompt_zero_shot,\n",
    "    partition=\"validation\",\n",
    ")\n",
    "\n",
    "# store tuning dataset in GCS\n",
    "gemini_validation_data_gcs_path = f\"gs://{BUCKET_NAME}/tuning_experiments/validation_dataset_gemini.jsonl\"  # @param {type: \"string\"}\n",
    "df_gemini_validation.to_json(\n",
    "    gemini_validation_data_gcs_path, orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpuZz3jCfcuD"
   },
   "source": [
    "####  [Option 3] AutoML JSONL training dataset format to Gemini tuning data format\n",
    "\n",
    "If you were previously using Vertex AI AutoML for text classification, and you have your data in the below JSONL format expected by AutoML:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"classificationAnnotation\": {\n",
    "    \"displayName\": \"label\"\n",
    "  },\n",
    "  \"textContent\": \"inline_text\",\n",
    "  \"dataItemResourceLabels\": {\n",
    "    \"aiplatform.googleapis.com/ml_use\": \"training|test|validation\"\n",
    "  }\n",
    "}\n",
    "{\n",
    "  \"classificationAnnotation\": {\n",
    "    \"displayName\": \"label2\"\n",
    "  },\n",
    "  \"textContent\": \"inline_text\",\n",
    "  \"dataItemResourceLabels\": {\n",
    "    \"aiplatform.googleapis.com/ml_use\": \"training|test|validation\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "You can utilize the below function to convert your AutoML JSONL datasets for text classification to the format expected for the training datasets to fine-tune Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJVikleQfcuD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_automl_jsonl_dataset_to_gemini(\n",
    "    automl_gcs_jsonl_path: str, system_prompt: str = \"\", partition=\"training\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts an AutoML JSONL dataset for text classification to the Gemini tuning format.\n",
    "\n",
    "    This function takes an AutoML JSONL dataset path on Google Cloud Storage and a partition name as input.\n",
    "    It reads the JSONL file, filters the data based on the partition, and then\n",
    "    converts it into the Gemini tuning format. The Gemini tuning format requires a\n",
    "    list of dictionaries, where each dictionary represents a conversation turn\n",
    "    with \"role\" and \"content\" keys.\n",
    "\n",
    "    Args:\n",
    "        automl_gcs_jsonl_path: The GCS path to the AutoML JSONL dataset for text classification.\n",
    "        system_prompt: the instructions to the model\n",
    "        partition: The partition to extract from the dataset (e.g., \"training\",\n",
    "        \"validation\", \"test\"). Defaults to \"training\".\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the data in the Gemini tuning format.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    with open(automl_gcs_jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            reformatted_data = dict()\n",
    "            reformatted_data[\"label\"] = data[\"classificationAnnotation\"][\"displayName\"]\n",
    "            reformatted_data[\"text\"] = data[\"textContent\"]\n",
    "            reformatted_data[\"partition\"] = data[\"dataItemResourceLabels\"][\n",
    "                \"aiplatform.googleapis.com/ml_use\"\n",
    "            ]\n",
    "            processed_data.append(reformatted_data)\n",
    "\n",
    "    df = pd.DataFrame(gemini_dataset)\n",
    "    df_automl = df.loc[df[\"partition\"] == partition]\n",
    "    gemini_dataset = list()\n",
    "\n",
    "    if system_prompt != \"\":\n",
    "        for idx, row in df_automl.iterrows():\n",
    "            message_system = {\"role\": \"system\", \"content\": system_prompt}\n",
    "            message_user = {\"role\": \"user\", \"content\": row[\"text\"]}\n",
    "            message_model = {\"role\": \"model\", \"content\": row[\"label\"]}\n",
    "            gemini_dataset.append(\n",
    "                {\"messages\": [message_system, message_user, message_model]}\n",
    "            )\n",
    "    else:\n",
    "        for idx, row in df_automl.iterrows():\n",
    "            message_user = {\"role\": \"user\", \"content\": row[\"text\"]}\n",
    "            message_model = {\"role\": \"model\", \"content\": row[\"label\"]}\n",
    "            gemini_dataset.append({\"messages\": [message_user, message_model]})\n",
    "\n",
    "    df_gemini = pd.DataFrame(gemini_dataset)\n",
    "\n",
    "    return df_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0QMNGCvBR4f"
   },
   "outputs": [],
   "source": [
    "## Usage Example for Training dataset\n",
    "gcs_path_automl_dataset = (\n",
    "    \"gs://<your-bucket-path>/<your-data>.jsonl\"  # @param {type: \"string\"}\n",
    ")\n",
    "df_gemini_tuning = convert_automl_jsonl_dataset_to_gemini(\n",
    "    automl_gcs_jsonl_path=gcs_path_automl_dataset,\n",
    "    system_prompt=system_prompt_zero_shot,\n",
    "    partition=\"training\",\n",
    ")\n",
    "\n",
    "# store tuning dataset in GCS\n",
    "gemini_tuning_data_gcs_path = f\"gs://{BUCKET_NAME}/tuning_experiments/tuning_dataset_gemini.jsonl\"  # @param {type: \"string\"}\n",
    "df_gemini_tuning.to_json(gemini_tuning_data_gcs_path, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWPTSFceBSJH"
   },
   "outputs": [],
   "source": [
    "## Usage Example for validation dataset\n",
    "gcs_path_automl_dataset = (\n",
    "    \"gs://<your-bucket-path>/<your-data>.csv\"  # @param {type: \"string\"}\n",
    ")\n",
    "df_gemini_validation = convert_automl_jsonl_dataset_to_gemini(\n",
    "    automl_gcs_jsonl_path=gcs_path_automl_dataset,\n",
    "    system_prompt=system_prompt_zero_shot,\n",
    "    partition=\"validation\",\n",
    ")\n",
    "\n",
    "# store tuning dataset in GCS\n",
    "gemini_validation_data_gcs_path = f\"gs://{BUCKET_NAME}/tuning_experiments/validation_dataset_gemini.jsonl\"  # @param {type: \"string\"}\n",
    "df_gemini_validation.to_json(\n",
    "    gemini_validation_data_gcs_path, orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51vqjijWfcuD"
   },
   "source": [
    "### 4.2 Start fine-tuning job\n",
    "\n",
    "- source_model: Specifies the base Gemini model version you want to fine-tune.\n",
    "- train_dataset: Path to your training data in JSONL format.\n",
    "\n",
    "Optional parameters\n",
    "\n",
    "- validation_dataset: If provided, this data is used to evaluate the model during tuning.\n",
    "- adapter_size: A higher adapter size means more trainable parameters.\n",
    "- epochs: The number of training epochs to run.\n",
    "- learning_rate_multiplier: A value to scale the learning rate during training.\n",
    "\n",
    "We recommend to make a different set of experiments with different hyperparameter.  The below configurations are recommended to experiment based on our experiments, if your dataset is in the size of 1000s and you are including the system role in your dataset.\n",
    "\n",
    "1. epochs: 4, learning_rate_multiplier: 1, adapter_size: 1\n",
    "1. epochs: 12, learning_rate_multiplier: 4,  adapter_size: 1\n",
    "\n",
    "If you are not including system role in your dataset, and only role user with the raw text and role models with the label, then we recommend to increase the adapter size. The below are some configurations you can start experimenting with.\n",
    "\n",
    "1. epochs: 12, learning_rate_multiplier: 4, adapter_size: 4\n",
    "1. epochs: 24, learning_rate_multiplier: 4,  adapter_size: 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhfUjyF2fcuD"
   },
   "outputs": [],
   "source": [
    "# Tune a model using `train` method.\n",
    "\n",
    "tuned_model_name = \"<add-name-for-tuned-model>\"  # @param {type: \"string\"}\n",
    "epochs = 4  # @param\n",
    "learning_rate_multiplier = 1  # @param\n",
    "adapter_size = 1  # @param\n",
    "\n",
    "\n",
    "sft_tuning_job = sft.train(\n",
    "    tuned_model_display_name=tuned_model_name,\n",
    "    source_model=\"gemini-1.0-pro-002\",\n",
    "    train_dataset=tuning_data_gcs_path,\n",
    "    # Optional:\n",
    "    validation_dataset=validation_data_gcs_path,\n",
    "    epochs=epochs,\n",
    "    learning_rate_multiplier=learning_rate_multiplier,\n",
    "    adapter_size=adapter_size,\n",
    ")\n",
    "\n",
    "# Get the tuning job info.\n",
    "sft_tuning_job.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIgb_lalfcuD"
   },
   "outputs": [],
   "source": [
    "# Get the resource name of the tuning job\n",
    "sft_tuning_job_name = sft_tuning_job.resource_name\n",
    "sft_tuning_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xxo0CWRfcuD"
   },
   "source": [
    "### 4.3 Get the tuned model and test it is working as expected\n",
    "\n",
    "You can get the full path by going to your tuning jobs in the console and then to details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bHSuQEZfcuD"
   },
   "outputs": [],
   "source": [
    "# Get tuning job\n",
    "TUNING_JOB_ID = \"<add your tuning job id>\"  # @param\n",
    "sft_tuning_job = sft.SupervisedTuningJob(\n",
    "    f\"projects/{PROJECT_ID}/locations/{LOCATION}/tuningJobs/{TUNING_JOB_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahcGD1KxfcuD"
   },
   "outputs": [],
   "source": [
    "# tuned model endpoint name\n",
    "tuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name\n",
    "tuned_model_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LchxG2QufcuD"
   },
   "outputs": [],
   "source": [
    "# tuned model name\n",
    "tuned_model_name = sft_tuning_job.tuned_model_name\n",
    "tuned_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOzD7dG2fcuE"
   },
   "outputs": [],
   "source": [
    "tuned_model = GenerativeModel(\n",
    "    tuned_model_endpoint_name, system_instruction=[system_prompt_zero_shot]\n",
    ")\n",
    "\n",
    "response = tuned_model.generate_content([test[\"text\"].iloc[4]], stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZw7iucdfcuE"
   },
   "outputs": [],
   "source": [
    "print(\"predicted\", response.text)\n",
    "print(\"ground truth\", test[\"label_text\"].iloc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbOY6eEgfcuE"
   },
   "source": [
    "### 4.4 Run evaluations on tuned model and log experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8tVNr4SfcuE"
   },
   "outputs": [],
   "source": [
    "# Get the list of messages to predict\n",
    "messages_to_predict = test[\"text\"].to_list()\n",
    "# Compute the predictions using the zero-shot prompt\n",
    "predictions_tuned_model = batch_predict(\n",
    "    messages=messages_to_predict,\n",
    "    model_name=tuned_model_endpoint_name,\n",
    "    system_prompt=system_prompt_zero_shot,\n",
    "    temperature=0,\n",
    "    max_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JX-t7KXxfcuE"
   },
   "outputs": [],
   "source": [
    "df_evals[\"tuned-gem1.0-ep6-lrm1-rank4\"] = predictions_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ri9X1ZOfcuE"
   },
   "outputs": [],
   "source": [
    "metrics_tuned_gemini = evaluate_predictions(\n",
    "    df_evals.copy(),\n",
    "    target_column=\"label_text\",\n",
    "    predictions_column=\"tuned-gem1.0-ep6-lrm1-rank4\",\n",
    "    postprocessing=True,\n",
    ")\n",
    "metrics_tuned_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0DWM0nmfcuE"
   },
   "outputs": [],
   "source": [
    "# Log Experiment with zero-shot Prompt with Gemini 1.5 Pro\n",
    "\n",
    "params = {\n",
    "    \"model\": \"<your_tuned_model>\",\n",
    "    \"adaptation_type\": \"fine-tuning gemini 1.0 Pro 002\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_output_tokens\": 10,\n",
    "    \"epochs\": epochs,\n",
    "    \"lrm\": learning_rate_multiplier,  # learning rate multiplier\n",
    "    \"adapter_size\": adapter_size,\n",
    "}\n",
    "\n",
    "log_experiment_run_vertexai(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    run_name=\"<your-experiment-run-name\",\n",
    "    metrics=metrics_tuned_gemini,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cIFYQ-3fcuE"
   },
   "source": [
    "## 5.  Evaluation comparisons\n",
    "\n",
    "To assess the performance of your experiments in Vertex AI, you have two primary options. You can programmatically retrieve a comprehensive table (pandas DataFrame) containing all experiments and their associated metrics for in-depth analysis. Alternatively, Vertex AI offers a user-friendly visual UI enabling you to compare experiments, select specific runs for side-by-side comparisons, and gain rapid insights. For detailed instructions on both approaches, refer to the [Vertex AI documentation on evaluation comparisons](https://cloud.google.com/vertex-ai/docs/experiments/compare-analyze-runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vF948vqJfcuE"
   },
   "outputs": [],
   "source": [
    "df_experiments = get_experiments_data_frame_sample(\n",
    "    experiment=EXPERIMENT_NAME, project=PROJECT_ID, location=LOCATION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dtTMBKnfcuE"
   },
   "outputs": [],
   "source": [
    "df_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUNM5LJA1OZq"
   },
   "source": [
    "\n",
    "\n",
    "> In our experiments with this dataset the most performant model was achieved by tuning Gemini 1.0 Pro @002, with the below parameters:\n",
    "\n",
    "```\n",
    "epochs=6, learning_rate_multiplier= 1, and adapter_size=4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZzNV7fnuJsy"
   },
   "source": [
    "## [Optional] 6. Heuristics for Computing Confidence Scores\n",
    "\n",
    "Due to the multitask essence of LLMs computing confidence scores is not as straightforwad as it is with traditional predictive AI. Gemini models do not expose logprobs for the time being. However, the below snippets provide some options to use as a proxy for confidence scores in your predictions. You can expand these options to your own use cases and needs.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KW7wPWQWuQT4"
   },
   "source": [
    "### [Option 1] -  Getting multiple responses from the model and generate a majority voting ratio\n",
    "\n",
    "The overall idea is to generate different answers with the same model. Then pick the most \"voted/returned\" answer, and calculate its \"confidence score\" by dividing the number of votes among the total number of responses/candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rViPbiseuOmr"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_prediction_with_numeric_score(\n",
    "    text_to_predict: str, model: Any, candidate_counts\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates multiple predictions from a model and determines\n",
    "    the most frequent response along with its confidence score.\n",
    "\n",
    "    Args:\n",
    "        text_to_predict (str): The input text for which to generate predictions.\n",
    "        model (Any): The prediction model to use.\n",
    "        candidate_counts (int): The number of predictions to generate.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the majority prediction and its confidence score.\n",
    "              For example: {\"prediction\": \"business\", \"confidence_score\": 0.75}\n",
    "    \"\"\"\n",
    "    responses = list()\n",
    "    for i in range(candidate_counts):\n",
    "        responses.append(model.generate_content(text_to_predict).text)\n",
    "\n",
    "    counts = Counter(responses)\n",
    "    max_value = max(counts.values())\n",
    "    majority_response = [key for key in counts if counts[key] == max_value][0]\n",
    "    confidence = max_value / len(responses)\n",
    "    result = {\"prediction\": majority_response, \"confidence_score\": confidence}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IuzofkguVX6"
   },
   "outputs": [],
   "source": [
    "# Define the model and configurations\n",
    "generation_config = {\"max_output_tokens\": 10, \"temperature\": 0, \"candidate_count\": 1}\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "tuned_model_multiple_responses = GenerativeModel(\n",
    "    tuned_model_endpoint_name,\n",
    "    system_instruction=[system_prompt_zero_shot],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiN6cX3VuXeE"
   },
   "outputs": [],
   "source": [
    "res = get_prediction_with_numeric_score(\n",
    "    text_to_predict=test[\"text\"].iloc[473],\n",
    "    model=tuned_model_multiple_responses,\n",
    "    candidate_counts=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSVIIVHquYOL"
   },
   "outputs": [],
   "source": [
    "print(\"Predicted Response with Confidence Score: \\n\", res)\n",
    "print(\"Ground Truth:\\n\", test[\"label_text\"].iloc[473])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEWe0M-juZyx"
   },
   "source": [
    "### [Option 2] -  Generating \"Verbal Confidences\" with an LLM\n",
    "\n",
    "The idea is to make 2-calls per prediction, one for predicting the class, and a second one to ask the LLM to judge how confident it is about it, giving as options verbal confidences like \"low\", \"medium\" and \"high\".\n",
    "\n",
    "In this example, we will use our tuned gemini model to predict the class and frozen Gemini 1.5 Pro to judge the prediction verbally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGWzgnx1udB0"
   },
   "outputs": [],
   "source": [
    "def get_prediction_with_verbal_score(\n",
    "    text_to_predict: str,\n",
    "    model_to_predict_class: Any,\n",
    "    model_to_eval_prediction: Any,\n",
    "    possible_classes: list = [\"business\", \"entertainment\", \"sport\", \"tech\", \"politics\"],\n",
    "):\n",
    "    prediction = model_to_predict_class.generate_content(text_to_predict).text\n",
    "    remaining_classes = possible_classes.remove(prediction)\n",
    "    formatted_prompt = f\"\"\"\"TEXT:\n",
    "    {text_to_predict}\n",
    "\n",
    "    PREDICTED CLASS:\n",
    "    {prediction}\n",
    "\n",
    "    OTHER POSSIBLE CLASSES:\n",
    "    {remaining_classes}\n",
    "    \"\"\"\n",
    "    confidence = model_to_eval_prediction.generate_content(formatted_prompt).text\n",
    "    result = {\"prediction\": prediction, \"verbal_score\": confidence}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8ginB-BufDT"
   },
   "outputs": [],
   "source": [
    "# Define the configurations for the model which will conduct the class prediciton\n",
    "generation_config = {\"max_output_tokens\": 10, \"temperature\": 0, \"candidate_count\": 1}\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "model_to_predict_class = GenerativeModel(\n",
    "    tuned_model_endpoint_name,\n",
    "    system_instruction=[system_prompt_zero_shot],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USxu9I0nugwU"
   },
   "outputs": [],
   "source": [
    "# Define the configurations for the model which will evaluate the predictions\n",
    "\n",
    "eval_prompt = f\"\"\"\n",
    "You will get a text about a particular topic, the predicted class for the topic and a list of the other different classes that the model could have chosen.\n",
    "Your task is to judge how well the predicted class fitted the text, based on the other possible classes.\n",
    "You need to evaluate and judge your prediction, indicating how confidente you are with your answer. You will judge the prediction as follows:\n",
    "\n",
    "- If you are confident the text is correctly labeled with the given prediction, then respond with \"High\"\n",
    "- If it can be that the model could match other classes, or you are not very sure the class corresponds to the text, then respond with \"Medium\"\n",
    "- If you believe it makes no sense the class predicted for that text, then respond with \"Low\".\n",
    "\n",
    "You MUST only output \"High\", \"Medium\" or \"Low\" without any further explanation.\n",
    "\"\"\"\n",
    "\n",
    "model_to_eval_class = GenerativeModel(\n",
    "    \"gemini-1.5-pro-001\",\n",
    "    system_instruction=[eval_prompt],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf8liMlCumTl"
   },
   "outputs": [],
   "source": [
    "# Call the function to get the predictions with verbal score\n",
    "res_verbal_conf = get_prediction_with_verbal_score(\n",
    "    text_to_predict=test[\"text\"].iloc[473],\n",
    "    model_to_predict_class=tuned_model_multiple_responses,\n",
    "    model_to_eval_prediction=model_to_eval_class,\n",
    "    possible_classes=[\"business\", \"entertainment\", \"sport\", \"tech\", \"politics\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUHITtnXukCx"
   },
   "outputs": [],
   "source": [
    "print(\"Predicted Response with Verbal Score: \\n\", res_verbal_conf)\n",
    "print(\"Ground Truth:\\n\", test[\"label_text\"].iloc[473])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPX8n4Wp0_uN"
   },
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-SUNrp_1MJ3"
   },
   "source": [
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "Refer to this [instructions](https://cloud.google.com/vertex-ai/docs/tutorials/image-classification-custom/cleanup#delete_resources) to delete the resources from console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzpTkkai1LG7"
   },
   "outputs": [],
   "source": [
    "# Delete Experiment.\n",
    "delete_experiments = True\n",
    "if delete_experiments:\n",
    "    experiments_list = aiplatform.Experiment.list()\n",
    "    for experiment in experiments_list:\n",
    "        if experiment.resource_name == EXPERIMENT_NAME:\n",
    "            print(experiment.resource_name)\n",
    "            experiment.delete()\n",
    "            break\n",
    "\n",
    "print(\"***\" * 10)\n",
    "\n",
    "# Delete Endpoint.\n",
    "delete_endpoint = True\n",
    "# If force is set to True, all deployed models on this\n",
    "# Endpoint will be first undeployed.\n",
    "if delete_endpoint:\n",
    "    for endpoint in aiplatform.Endpoint.list():\n",
    "        if endpoint.resource_name == tuned_model_endpoint_name:\n",
    "            print(endpoint.resource_name)\n",
    "            endpoint.delete(force=True)\n",
    "            break\n",
    "\n",
    "print(\"***\" * 10)\n",
    "\n",
    "# Delete Model.\n",
    "delete_model = True\n",
    "if delete_model:\n",
    "    # Remove version from model name.\n",
    "    tuned_model_name = tuned_model_name.split(\"@\")[0]\n",
    "    for model in aiplatform.Model.list():\n",
    "        if model.resource_name == tuned_model_name:\n",
    "            print(model.resource_name)\n",
    "            model.delete()\n",
    "            break\n",
    "\n",
    "print(\"***\" * 10)\n",
    "\n",
    "# Delete Cloud Storage Bucket.\n",
    "delete_bucket = True\n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
